{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wumingna\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import pprint\n",
    "from tensorflow.compat.v1 import logging\n",
    "from tensorflow.compat.v1 import gfile\n",
    "\n",
    "input_descroption_file = '../../datasets/image_caption_data/results_20130124.token'\n",
    "input_img_feature_dir = '../../datasets/image_caption_data/feature_extraction_inception_v3/'\n",
    "input_vocab_file = '../../datasets/image_caption_data/vocab.txt'\n",
    "output_dir = '../../datasets/image_caption_data/local_run'\n",
    "\n",
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hps\n"
     ]
    }
   ],
   "source": [
    "class default_params():\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.num_vocab_word_threshold = 3\n",
    "        self.num_timesteps = 10\n",
    "        self.num_embedding_node = 32\n",
    "        self.num_lstm_node = [64,64]\n",
    "        self.num_lstm_layer = 2\n",
    "        self.num_fc_node = 32\n",
    "        self.batch_size = 100\n",
    "        self.cell_type = 'lstm'\n",
    "        self.clip_lstm_grads = 1.0\n",
    "        self.learning_rate = 0.001\n",
    "        self.keep_prob = 0.8\n",
    "        self.log_frequent = 500\n",
    "        self.save_frequent = 1000\n",
    "\n",
    "num_train_step = 5000\n",
    "hps = default_params('hps')\n",
    "print(hps.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab size:10875\n",
      "[3838, 4863, 1, 30]\n",
      "'the white A in and is'\n"
     ]
    }
   ],
   "source": [
    "class Vocab():\n",
    "    def __init__(self,filename,num_vocab_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._id_to_word ={}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._num_vocab_word_threshold = num_vocab_word_threshold\n",
    "        self.read_file(filename)\n",
    "    \n",
    "    def read_file(self,filename):\n",
    "        with gfile.GFile(filename,'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word,frequent = line.strip('\\r\\n').split('\\t')\n",
    "            frequent = int(frequent)\n",
    "            if frequent < self._num_vocab_word_threshold:\n",
    "                continue\n",
    "            else:\n",
    "                idx = len(self._word_to_id)\n",
    "                if word == '<UNK>':\n",
    "                    self._nuk = idx\n",
    "                elif word == '.':\n",
    "                    self._eos = idx\n",
    "                if idx in self._word_to_id or word in self._id_to_word:\n",
    "                    raise Exception(\"wrong...\")\n",
    "                self._word_to_id[word] = idx\n",
    "                self._id_to_word[idx] = word \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "    \n",
    "    def word_to_id(self,word):\n",
    "        return self._word_to_id.get(word,self.unk)\n",
    "    \n",
    "    def id_to_word(self,idx):\n",
    "        return self._id_to_word.get(idx,'<UNK>')\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    \n",
    "    def encode(self,sentence):\n",
    "        word_ids = [self.word_to_id(word) for word in sentence.split(' ')]\n",
    "        return word_ids\n",
    "\n",
    "    def decode(self,sentence_ids):\n",
    "        id_words = [self.id_to_word(id) for id in sentence_ids]\n",
    "        return ' '.join(id_words)\n",
    "\n",
    "vocab = Vocab(input_vocab_file,hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "logging.info('vocab size:%d' % vocab_size)\n",
    "pprint.pprint(vocab.encode('i am a girl'))\n",
    "pprint.pprint(vocab.decode([5,20,3,4,7,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_token_file(token_file):\n",
    "    image_name_to_token = {}\n",
    "    with gfile.GFile(token_file,'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        image_id , description = line.strip('\\r\\n').split('\\t')\n",
    "        image_name,_ = image_id.split('#')\n",
    "        image_name_to_token.setdefault(image_name,[])\n",
    "        image_name_to_token[image_name].append(description)\n",
    "    return image_name_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:31783\n",
      "['A man in jeans is reclining on a green metal bench along a busy sidewalk and '\n",
      " 'crowded street .',\n",
      " 'A white male with a blue sweater and gray pants laying on a sidewalk bench .',\n",
      " 'A man in a blue shirt and gray pants is sleeping on a sidewalk bench .',\n",
      " 'A person is sleeping on a bench , next to cars .',\n",
      " 'A man sleeping on a bench in a city area .']\n",
      "INFO:tensorflow:31783\n",
      "[[3, 9, 4, 132, 8, 3558, 6, 1, 48, 337, 146, 139, 1, 244, 93, 7, 380, 36, 2],\n",
      " [3, 20, 179, 11, 1, 26, 284, 7, 120, 128, 296, 6, 1, 93, 146, 2],\n",
      " [3, 9, 4, 1, 26, 21, 7, 120, 128, 8, 340, 6, 1, 93, 146, 2],\n",
      " [3, 63, 8, 340, 6, 1, 146, 12, 70, 15, 519, 2],\n",
      " [3, 9, 340, 6, 1, 146, 4, 1, 112, 171, 2]]\n"
     ]
    }
   ],
   "source": [
    "def conver_token_to_id(image_name_to_token):\n",
    "    image_name_to_token_id = {}\n",
    "    for images_name in image_name_to_token:\n",
    "        image_name_to_token_id.setdefault(images_name,[])\n",
    "        descriptions = image_name_to_token[images_name]\n",
    "        for description in descriptions:\n",
    "            description_ids = vocab.encode(description)\n",
    "            image_name_to_token_id[images_name].append(description_ids)\n",
    "    return image_name_to_token_id\n",
    "\n",
    "image_name_to_token = parse_token_file(input_descroption_file)\n",
    "image_name_to_token_id = conver_token_to_id(image_name_to_token)\n",
    "logging.info(len(image_name_to_token))\n",
    "pprint.pprint(image_name_to_token['2778832101.jpg'])\n",
    "logging.info(len(image_name_to_token_id))\n",
    "pprint.pprint(image_name_to_token_id['2778832101.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#获取数据\n",
    "class ImageCaptiondatas():\n",
    "    def __init__(self,image_name_to_token_id,img_feature_dir,num_timesteps,vocab,need_shuffle=True):\n",
    "        self._image_name_to_token_id = image_name_to_token_id\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._vocab = vocab\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0\n",
    "        self._all_image_feature_filepath = []\n",
    "        for filename in gfile.ListDirectory(img_feature_dir):\n",
    "            self._all_image_feature_filepath.append(os.path.join(img_feature_dir,filename))\n",
    "        self._img_feature_image_names = []\n",
    "        self._img_feature_data = []\n",
    "        self._load_img_feature_pickle()\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle()\n",
    "    \n",
    "    def _load_img_feature_pickle(self):\n",
    "        for filepath in self._all_image_feature_filepath:\n",
    "            with gfile.GFile(filepath,'rb') as f:\n",
    "                image_names,image_feature = pickle.load(f,encoding='iso-8859-1')\n",
    "                self._img_feature_image_names += image_names\n",
    "                self._img_feature_data.append(image_feature)\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data)\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        self._img_feature_data = np.reshape(self._img_feature_data,[origin_shape[0],origin_shape[3]])\n",
    "        self._img_feature_image_names = np.asarray(self._img_feature_image_names)\n",
    "    \n",
    "    def _shuffle(self):\n",
    "        p = np.random.permutation(len(self._img_feature_image_names))\n",
    "        self._img_feature_image_names = self._img_feature_image_names[p]\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "    \n",
    "    def image_size(self):\n",
    "        return len(self._img_feature_image_names)\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self._img_feature_data[-1]\n",
    "    \n",
    "    def choice_sentence(self,image_names):\n",
    "        batch_sentence_id = []\n",
    "        batch_weight=[]\n",
    "        for image_name in image_names:\n",
    "            sentence_ids = image_name_to_token_id[iamge_name]\n",
    "            choose_one = random.choice(sentence_ids)\n",
    "            sentence_length = len(choose_one)\n",
    "            weights = [1 for i in range(sentence_length)]\n",
    "            if sentence_length > self._num_timesteps:\n",
    "                choose_one = choose_one[0:self._num_timesteps]\n",
    "                weights = weights[0:self._num_timesteps]\n",
    "            else:\n",
    "                padding_length = self._num_timesteps - sentence_length\n",
    "                choose_one = choose_one\n",
    "                weights = weights+[0 for i in range(padding_length)]\n",
    "            batch_sentence_id.append(choose_one)\n",
    "            batch_weight.append(weights)\n",
    "        batch_sentence_id = np.asarray(batch_sentence_id)\n",
    "        batch_weight = np.asarray(batch_weight)\n",
    "        return batch_sentence_id,batch_weight\n",
    "    \n",
    "\n",
    "    def next(self,batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.image_size():\n",
    "            self._shuffle()\n",
    "            self._indicator = 0\n",
    "            self.end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator < self.image_size\n",
    "        \n",
    "        batch_image_name = self._img_feature_image_names[self._indicator:end_indicator]\n",
    "        batch_feature_data = self._img_feature_data[self._indicator:end_indicator]\n",
    "        \n",
    "        batch_image_sentence_id,batch_weight = self.choice_sentence(batch_image_name)\n",
    "        self._indicator = 0\n",
    "        return batch_image_name,batch_feature_data,batch_image_sentence_id,batch_weight\n",
    "\n",
    "caption_date = ImageCaptiondatas(image_name_to_token_id,input_img_feature_dir,hps.num_timesteps,vocab)\n",
    "image_feature_dim = caption_date.feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_rnn_cell(hidden_dim,cell_type):\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.nn.rnn_cell.BasicLSTMCell(hidden_dim,state_is_tuple=True)\n",
    "\n",
    "def dropout(cell,keep_prob):\n",
    "    return tf.nn.rnn_cell.DropoutWrapper(cell,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def creat_train_model(hps,vocab_size,feature_dim):\n",
    "    \n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    image_feature = tf.placeholder(tf.float32,[batch_size,feature_dim])\n",
    "    sentence = tf.placeholder(tf.float32,[batch_size,num_timesteps])\n",
    "    mask = tf.placeholder(tf.float32,[batch_size,num_timesteps])\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    \n",
    "    global_step = tf.Variable(tf.zeros([],tf.int64),name='global_step',trainable=False)\n",
    "    \n",
    "    init_embedding = tf.random_uniform_initializer(-1.0,1.0)\n",
    "    with tf.variable_scope('init_embedding',initializer=init_embedding):\n",
    "        matrix_embed = tf.get_variable(\n",
    "                                       'embedding',\n",
    "                                       [vocab_size,hps.num_embedding_node],\n",
    "                                        tf.float32)\n",
    "        embedding_id = tf.nn.embedding_lookup(matrix_embed,sentence[:,0:num_timesteps-1])\n",
    "    \n",
    "    \n",
    "    init_img_feature = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('init_img_feature',initializer=init_img_feature):\n",
    "        embed_img = tf.layers.dense(image_feature,hps.num_embedding_node)\n",
    "        embed_img = tf.expand_dims(embed_img,1)\n",
    "        embed_input = tf.concat([embed_img,embedding_id],axis=1)\n",
    "        \n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_init',initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layer):\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(hps.num_lstm_node[i],hps.cell_type)\n",
    "            cell = dropout(cell,hps.keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell =tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "        \n",
    "        init_state = cell.zero_state(batch_size,tf.float32)\n",
    "        \n",
    "        run_output,_ = tf.nn.dynamic_rnn(cell,\n",
    "                                         embed_input,\n",
    "                                         initial_state = init_state)\n",
    "        \n",
    "    init_fc = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('init_fc',initializer=init_fc):\n",
    "        runn_output_2d = tf.reshape(run_output,[-1,hps.num_lstm_node[-1]])\n",
    "        fc = tf.layers.dense(runn_output_2d,hps.num_fc_node,name='fc1')\n",
    "        fc_keep_prob = tf.nn.dropout(fc,hp.keep_prob)\n",
    "        fc1 = tf.nn.relu(fc_keep_prob)\n",
    "        logits = tf.layers.dense(fc1,vocab_size,name='logits')\n",
    "        \n",
    "    with tf.name_scope('matrix'):\n",
    "        sentence_flatten = tf.reshape(sentance,[-1])\n",
    "        mask_flatten = tf.reshape(mask,[-1])\n",
    "        mask_sum = tf.reduce_sum(mask_flatten)\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=sentence_flatten,logits=logits)\n",
    "        weighted_sotfmax_loss = tf.multiply(softmax_loss,tf.cast(mask_flatten,tf.float32))\n",
    "        \n",
    "        prediction = tf.argmax(logits,axis=1)\n",
    "        \n",
    "        correct = tf.equal(tf.cast(prediction,tf.float32),sentence_flatten)\n",
    "        correct_prediction_with_mask = tf.multiply(\n",
    "            tf.cast(correct, tf.float32),\n",
    "            mask_flatten)\n",
    "        \n",
    "        accuracy = tf.reduce_sum(correct_prediction_with_mask) / mask_sum\n",
    "        \n",
    "        loss = tf.reduce_sum(weighted_sotfmax_loss) / mask_sum\n",
    "\n",
    "        tf.summary.scalar('loss',loss)\n",
    "    \n",
    "    with tf.name_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads,_ = tf.clip_by_global_norm(tf.gradients(loss,tvars),hps.clip_lstm_grads)\n",
    "        for grad,var in zip(grads,tvars):\n",
    "            tf.summary.histogram('%s_grad:%d' % (var.name),grad)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads,tvars),global_step = global_step)\n",
    "        \n",
    "        return ((image_feature,sentence,mask,keep_prob),\n",
    "                (loss,accuracy,train_op),\n",
    "               (global_step))\n",
    "\n",
    "placeholder , matrix , others = creat_train_model(hps,vocab_size,image_feature_dim)\n",
    "\n",
    "image_feature,sentence,mask,keep_prob = placeholder\n",
    "\n",
    "loss,accuracy,train_op = matrix\n",
    "\n",
    "summery_op = tf.summary.merge_all()\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_dir,sess.graph)\n",
    "    for i in range(num_train_step):\n",
    "        batch_img_features, batch_sentence_ids, batch_weights, _ = caption_data.next(hps.batch_size)\n",
    "        input_vals = (batch_img_features, batch_sentence_ids, batch_weights, hps.keep_prob)\n",
    "        feed_dict = dict(zip(placeholder,input_vals))\n",
    "        fetches = [global_step,loss,accuracy,train_op]\n",
    "       \n",
    "        \n",
    "        should_log = (i+1) % hps.log_frequent == 0\n",
    "        should_save = (i+1) % hps.save_frequent == 0 \n",
    "        if should_log:\n",
    "            fetches = fetches+[summery_op]\n",
    "        result_val = sess.run(fetches,feed_dict)\n",
    "        global_step_val,loss_val,acc_val,_ = result_val\n",
    "        if should_log:\n",
    "            summary_str = result_val[4]\n",
    "            writer.add_summery(summery_str,global_step_val)\n",
    "        if should_save:\n",
    "            saver.save(sess,os.path.join(output_dir,'img_caption'),global_step_val)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
