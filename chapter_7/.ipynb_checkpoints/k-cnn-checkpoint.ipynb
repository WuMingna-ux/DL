{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wumingna\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#构件LSTM模型\n",
    "#训练代码\n",
    "#数据集封装代码\n",
    "#\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lstm 需要的参数\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_embedding_size = 16, # 每个词语的向量的长度\n",
    "\n",
    "        # 指定 lstm 的 步长， 一个sentence中会有多少个词语\n",
    "        # 因为执行的过程中是用的minibatch，每个batch之间还是需要对齐的\n",
    "        # 在测试时，可以是一个变长的\n",
    "        num_timesteps = 50, # 在一个sentence中 有 50 个词语\n",
    "\n",
    "        num_lstm_nodes = [32, 32], # 每一层的size是多少\n",
    "        num_lstm_layers = 2, # 和上句的len 是一致的\n",
    "        # 有 两层 神经单元，每一层都是 32 个 神经单元\n",
    "\n",
    "        num_fc_nodes = 32, # 全连接的节点数\n",
    "        batch_size = 100,\n",
    "        clip_lstm_grads = 1.0,\n",
    "        # 控制lstm的梯度，因为lstm很容易梯度爆炸或者消失\n",
    "        # 这种方式就相当于给lstm设置一个上限，如果超过了这个上限，就设置为这个值\n",
    "        learning_rate = 0.001,\n",
    "        num_word_threshold = 10, # 词频太少的词，对于模型训练是没有帮助的，因此设置一个门限\n",
    "    )\n",
    "\n",
    "\n",
    "hps = get_default_params() # 生成 参数 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hps = get_default_params()\n",
    "train_file = '../../datasets/text_classification_data/cnews.train.txt'\n",
    "test_file = '../../datasets/text_classification_data/cnews.test.txt'\n",
    "val_file = '../../datasets/text_classification_data/cnews.val.txt'\n",
    "\n",
    "vocab_filr = '../../datasets/text_classification_data/cnews.vocab.txt'\n",
    "lable_file = '../../datasets/text_classification_data/cnews.label.txt'\n",
    "\n",
    "output_file = '../../datasets/text_classification_data/rnn_result'\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    mkdir(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    '''\n",
    "    词表的封装\n",
    "    '''\n",
    "    def __init__(self, filename, num_word_threahold):\n",
    "        # 每一个词，给她一个id，另外还要统计词频。ps：前面带下划线的为私有成员\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1 # 先给 unk 赋值一个 负值，然后根据实际情况在赋值\n",
    "        self._num_word_theshold = num_word_threahold #　低于　这个值　就忽略掉该词\n",
    "        self._read_dict(filename) # 读词表方法\n",
    "\n",
    "    def _read_dict(self, filename):\n",
    "        '''\n",
    "        读这个词表\n",
    "        :param filename: 路径\n",
    "        :return: none\n",
    "        '''\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\n').split('\\t')\n",
    "            word = word # 获得　单词\n",
    "            frequency = int(frequency) # 获得　频率\n",
    "            if frequency < self._num_word_theshold:\n",
    "                continue # 门限过滤一下\n",
    "            idx = len(self._word_to_id) #这里使用了一个id递增的小技巧\n",
    "            if word == '<UNK>': # 如果是空格，就把上一个id号给它\n",
    "                # 如果是 unk的话， 就特殊处理一下\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "            # 如果 word 存在，就把 idx 当做值，将其绑定到一起\n",
    "            # 如果 word 在词表中不存在，就把nuk的值赋予它\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        '''\n",
    "        为单词分配id值\n",
    "        :param word: 单词\n",
    "        :return:\n",
    "        '''\n",
    "        # 字典.get() 如果有值，返回值；无值，返回默认值（就是第二个参数）\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "\n",
    "    def sentence_to_id(self, sentence):\n",
    "        '''\n",
    "        将句子 转换成 id 向量\n",
    "        :param sentence: 要输入的句子（分词后的句子）\n",
    "        :return:\n",
    "        '''\n",
    "        # 单条句子的id vector\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "        # cur_word 有可能不存在，需要使用函数进行过滤一下\n",
    "        return word_ids\n",
    "\n",
    "    # 定义几个 访问私有成员属性的方法\n",
    "    # Python内置的 @ property装饰器就是负责把一个方法变成属性调用的\n",
    "    @ property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CategoryDict:\n",
    "    '''\n",
    "    和 词表的 方法 几乎一样\n",
    "    '''\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "\n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Exception('%s is not in our category list' % category)\n",
    "        return self._category_to_id[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#　获得　词表　对象\n",
    "vocab = Vocab(vocab_file, hps.num_word_threshold)\n",
    "# 词表长度\n",
    "vocab_size = vocab.size()\n",
    "\n",
    "\n",
    "# 获得　类别表　对象\n",
    "category_vocab = CategoryDict(category_file)\n",
    "# 类别 总数\n",
    "num_classes = category_vocab.size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 封装数据集\n",
    "class TextDataSet:\n",
    "    '''\n",
    "    数据集 封装\n",
    "    功能： 1、将数据集向量化。2、返回batch\n",
    "    '''\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        '''\n",
    "        封装数据集\n",
    "        :param filename: 可以是训练数据集、测试数据集、验证数据集等\n",
    "        :param vocab: 词表 对象\n",
    "        :param category_vocab: 类别 对象\n",
    "        :param num_timesteps: 步长 （sentence的总长度）\n",
    "        '''\n",
    "        # 将　各个对象　赋值\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "\n",
    "        # matrix\n",
    "        self._inputs = []\n",
    "        # vector\n",
    "        self._outputs = []\n",
    "        # batch 起始点\n",
    "        self._indicator = 0\n",
    "\n",
    "        # 将文本数据　解析　成　matrix\n",
    "        self._parse_file(filename) # 进行解析\n",
    "\n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from %s', filename)\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\n').split('\\t')\n",
    "\n",
    "            # 得到 一个 label 的 id\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "            # 得到 一个 vector\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "\n",
    "            # 需要在每一个minibatch上进行对齐，对 word 进行 对齐 操作\n",
    "            # 如果 超出了界限，就 截断， 如果 不足，就 填充\n",
    "            id_words = id_words[0: self._num_timesteps] # 超过了 就 截断\n",
    "            # 低于 num_timesteps 就填充,也就是说，上一句和下面两句 可以完全并列写，神奇！！\n",
    "            # 这里的编码方式感觉很巧妙！！！\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [self._vocab.unk for i in range(padding_num)]\n",
    "\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "\n",
    "        # 转变为 numpy 类型\n",
    "        self._inputs = np.asarray(self._inputs, dtype=np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype=np.int32)\n",
    "        # 对数据进行随机化\n",
    "        self._random_shuffle()\n",
    "        self._num_sample = len(self._inputs)\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception('batch_size: %d is too large' % batch_size)\n",
    "\n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_outputs           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 得到 三个 文本对象，当中都包含了 input 和 label\n",
    "train_dataset = TextDataSet(train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "val_dataset = TextDataSet(val_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(test_file, vocab, category_vocab, hps.num_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始计算图模型 （重点）\n",
    "def create_model(hps, vocab_size, num_classes):\n",
    "    '''\n",
    "    构建lstm\n",
    "    :param hps: 参数对象\n",
    "    :param vocab_size:  词表 长度\n",
    "    :param num_classes:  分类数目\n",
    "    :return:\n",
    "    '''\n",
    "    num_timesteps = hps.num_timesteps # 一个句子中 有 num_timesteps 个词语\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    # 设置两个 placeholder， 内容id 和 标签id\n",
    "    inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    outputs = tf.placeholder(tf.int32, (batch_size, ))\n",
    "\n",
    "    # dropout keep_prob 表示要keep多少值，丢掉的是1-keep_prob\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64),\n",
    "        name='global_step',\n",
    "        trainable = False)  # 可以保存 当前训练到了 哪一步，而且不训练\n",
    "\n",
    "    # 随机的在均匀分布下初始化, 构建 embeding 层\n",
    "    embeding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "\n",
    "    # 和 name_scope 作用是一样的，他可以定义指定 initializer\n",
    "    # tf.name_scope() 和 tf.variable_scope() 的区别 参考：\n",
    "    # https://www.cnblogs.com/adong7639/p/8136273.html\n",
    "    with tf.variable_scope('embedding', initializer=embeding_initializer):\n",
    "        # tf.varialble_scope() 一般 和 tf.get_variable() 进行配合\n",
    "        # 构建一个 embedding 矩阵,shape 是 [词表的长度, 每个词的embeding长度 ]\n",
    "        embeddings = tf.get_variable('embedding', [vocab_size, hps.num_embedding_size], tf.float32)\n",
    "\n",
    "        # 每一个词，都要去embedding中查找自己的向量\n",
    "        # [1, 10, 7] 是一个句子，根据 embedding 进行转化\n",
    "        # 如： [1, 10, 7] -> [embedding[1], embedding[10], embedding[7]]\n",
    "        embeding_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "        # 上句的输入： Tensor(\"embedding/embedding_lookup:0\", shape=(100, 50, 16), dtype=float32)\n",
    "        # 输出是一个三维矩阵，分别是：100 是 batch_size 大小，50 是 句子中的单词数量，16 为 embedding 向量长度\n",
    "\n",
    "\n",
    "    # lstm 层\n",
    "\n",
    "    # 输入层 大小 加上 输出层的大小，然后开方\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "\n",
    "\n",
    "\n",
    "    def _generate_parames_for_lstm_cell(x_size, h_size, bias_size):\n",
    "        '''\n",
    "        生成参数的变量\n",
    "        :param x_size: x × w  其中 w 的形状\n",
    "        :param h_size: 上一层 输出h 的形状\n",
    "        :param bias_size: 偏置的形状\n",
    "        :return: 各个 变量\n",
    "        '''\n",
    "        x_w = tf.get_variable('x_weights', x_size) # 输入x的w权重的值\n",
    "        h_w = tf.get_variable('h_weights', h_size) # 上一层 输出h 的 值\n",
    "        b = tf.get_variable('biases', bias_size, initializer=tf.constant_initializer(0.0)) # 偏置的 值\n",
    "\n",
    "        return x_w, h_w, b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with tf.variable_scope('lstm_nn', initializer = lstm_init):\n",
    "        '''\n",
    "        cells = [] # 保存两个lstm层\n",
    "        # 循环这两层 lstm\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            # BasicLSTMCell类是最基本的LSTM循环神经网络单元。\n",
    "            # 输入参数和BasicRNNCell差不多， 设置一层 的 lstm 神经元\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                hps.num_lstm_nodes[i], # 每层的 节点个数\n",
    "                state_is_tuple = True # 中间状态是否是一个元组\n",
    "            )\n",
    "            cell = tf.contrib.rnn.DropoutWrapper( #　进行　dropout\n",
    "                cell,\n",
    "                output_keep_prob = keep_prob # dropout 的 比例\n",
    "            )\n",
    "            cells.append(cell)\n",
    "\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        # 该方法的作用是：将两层的lstm 连到一起，比如：上层的输出是下层的输入\n",
    "        # 此时的cell，已经是一个多层的lstm，但是可以当做单层的来操作，比较简单\n",
    "\n",
    "        # 保存中间的一个隐含状态，隐含状态在初始化的时候初始化为0，也就是零矩阵\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        #　rnn_outputs: [batch_size, num_timesteps, lstm_outputs[-1](最后一层的输出)]\n",
    "        # _ 代表的是隐含状态\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(\n",
    "            cell, embeding_inputs, initial_state = initial_state\n",
    "        ) # 现在的rnn_outputs 代表了每一步的输出\n",
    "\n",
    "        # 获得最后一步的输出，也就是说，最后一个step的最后一层的输出\n",
    "        last = rnn_outputs[:, -1, :]\n",
    "        # print(last) Tensor(\"lstm_nn/strided_slice:0\", shape=(100, 32), dtype=float32)\n",
    "\n",
    "\n",
    "        '''\n",
    "        # 生成 四组 可变 参数，分别是 遗忘门、输入门、输出门  和 tanh\n",
    "        # 输入门\n",
    "        with tf.variable_scope('inputs'):\n",
    "            ix, ih, ib = _generate_parames_for_lstm_cell( # 以i开头，代表 inputs\n",
    "                x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]], # []\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "        with tf.variable_scope('outputs'):\n",
    "            ox, oh, ob = _generate_parames_for_lstm_cell( # 以i开头，代表 inputs\n",
    "                x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]], # []\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('forget'):\n",
    "            fx, fh, fb = _generate_parames_for_lstm_cell( # 以i开头，代表 inputs\n",
    "                x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]], # []\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "        with tf.variable_scope('memory'):\n",
    "            cx, ch, cb = _generate_parames_for_lstm_cell( # 以i开头，代表 inputs\n",
    "                x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]], # []\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "\n",
    "        # 初始化 隐状态 隐状态的形状 (batch_size, lstm最后一层神经个数)\n",
    "        state = tf.Variable(tf.zeros([batch_size, hps.num_lstm_nodes[0]]),\n",
    "                            trainable = False\n",
    "                            )\n",
    "\n",
    "        # 每个神经元的输出 形状同上\n",
    "        h = tf.Variable(tf.zeros([batch_size, hps.num_lstm_nodes[0]]),\n",
    "                        trainable = False\n",
    "                        )\n",
    "\n",
    "        for i in range(num_timesteps): # 按照 词语的数量 进行\n",
    "            # [batch_size, 1, embed_size]\n",
    "            embeding_input = embeding_inputs[:, i, :] # 取出 句子中 的 第一个词语(当i为1时)\n",
    "            #这样每次取出来的 中间的那一维度 就是 1，可以将其合并掉\n",
    "\n",
    "            # 因为是 只有一个词语，所以将其reshape成 二维\n",
    "            embeding_input = tf.reshape(embeding_input, [batch_size, hps.num_embedding_size])\n",
    "\n",
    "            # 遗忘门\n",
    "            forget_gate = tf.sigmoid(\n",
    "                # 输入x与w相乘，加上 上一层输出h与hw相乘，在加上，偏置\n",
    "                # 以下各个门同理\n",
    "                tf.matmul(embeding_input, fx) + tf.matmul(h, fh) + fb\n",
    "            )\n",
    "\n",
    "            # 输入门\n",
    "            input_gate = tf.sigmoid(\n",
    "                tf.matmul(embeding_input, ix) + tf.matmul(h, ih) + ib\n",
    "            )\n",
    "\n",
    "            # 输出门\n",
    "            output_gate = tf.sigmoid(\n",
    "                tf.matmul(embeding_input, ox) + tf.matmul(h, oh) + ob\n",
    "            )\n",
    "\n",
    "            # tanh 层\n",
    "            mid_state = tf.tanh(\n",
    "                tf.matmul(embeding_input, cx) + tf.matmul(h, ch) + cb\n",
    "            )\n",
    "\n",
    "\n",
    "            # c状态 是 上一个单元传入c状态×遗忘门 再加上 输入门×tanh\n",
    "            state = mid_state * input_gate + state * forget_gate\n",
    "\n",
    "            h = output_gate * tf.tanh(state)\n",
    "        last = h # 只需要 最后一个 输出 就可以了\n",
    "        # 输出 Tensor(\"lstm_nn/mul_149:0\", shape=(100, 32), dtype=float32)\n",
    "        # 和注释部分的 last 输出 是同样的结果\n",
    "\n",
    "\n",
    "    # 将最后一层的输出 链接到一个全连接层上\n",
    "    # 参考链接：https://www.w3cschool.cn/tensorflow_python/tensorflow_python-fy6t2o0o.html\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer = fc_init): # initializer 此范围内变量的默认初始值\n",
    "        fc1 = tf.layers.dense(last,\n",
    "                              hps.num_fc_nodes,\n",
    "                              activation = tf.nn.relu,\n",
    "                              name = 'fc1')\n",
    "        # 进行 dropout\n",
    "        fc1_dropout = tf.nn.dropout(fc1, keep_prob)\n",
    "        # 进行更换 参考：https://blog.csdn.net/UESTC_V/article/details/79121642\n",
    "\n",
    "        logits = tf.layers.dense(fc1_dropout, num_classes, name='fc2')\n",
    "\n",
    "    # 没有东西需要初始化，所以可以直接只用name_scope()\n",
    "    with tf.name_scope('metrics'):\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = logits,\n",
    "            labels = outputs\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # 该方法 做了三件事：1,labels 做 onehot，logits 计算softmax概率，3. 做交叉熵\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "\n",
    "        #\n",
    "        y_pred = tf.argmax(\n",
    "            tf.nn.softmax(logits),\n",
    "            1,\n",
    "            #output_type = tf.int64\n",
    "        )\n",
    "\n",
    "\n",
    "        # 这里做了 巨大 修改，如果问题，优先检查这里！！！！！！\n",
    "        #print(type(outputs), type(y_pred))\n",
    "        correct_pred = tf.equal(outputs, tf.cast(y_pred, tf.int32)) # 这里也做了修改\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    with tf.name_scope('train_op'):\n",
    "        tvars = tf.trainable_variables() # 获取所有可以训练的变量\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name: %s' % (var.name)) # 打印出所有可训练变量\n",
    "\n",
    "        # 对 梯度进行 截断.\n",
    "        # grads是截断之后的梯度\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), # 在可训练的变量的梯度\n",
    "            hps.clip_lstm_grads\n",
    "        ) # 可以 获得 截断后的梯度\n",
    "\n",
    "\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate) # 将每个梯度应用到每个变量上去\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), # 将 梯度和参数 绑定起来\n",
    "            global_step = global_step # 这个参数 等会儿，再好好研究一下\n",
    "        )\n",
    "\n",
    "\n",
    "    return ((inputs, outputs, keep_prob),\n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
