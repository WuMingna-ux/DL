{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wumingna\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "\n",
    "#生成超参数\n",
    "class Default_params:\n",
    "    def __init__(self,name):\n",
    "        self.num_embedding_size = 16\n",
    "        self.num_timesteps = 50\n",
    "        self.num_lstm_nodes = [32, 32]\n",
    "        self.num_lstm_layers = 2\n",
    "        self.num_fc_nodes = 32\n",
    "        self.batch_size = 100\n",
    "        self.clip_lstm_grads = 1.0\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_word_threshold = 10\n",
    "        self.name = name\n",
    "      \n",
    "\n",
    "hps = Default_params('hps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#设置文件路径\n",
    "train_file = './news_data/cnews.train.seg.txt'\n",
    "val_file = './news_data/cnews.val.seg.txt'\n",
    "test_file = './news_data/cnews.test.seg.txt'\n",
    "vocab_file = './news_data/cnews.vocab.txt' # 统计的词频\n",
    "category_file = './news_data/cnews.category.txt' # 标签\n",
    "output_folder = './news_data/run_text_rnn'\n",
    "\n",
    "if not os.path.join(output_folder):\n",
    "    mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#从句子中提取单词\n",
    "class Vocab():\n",
    "    def __init__(self,filename,num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk =-1\n",
    "        self._num_word = num_word\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "    \n",
    "    def read_dict(self,filename):\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                word,frequent = line.strip('\\r\\n').split('\\t')\n",
    "                if frequent < self._num_word_threshold:\n",
    "                    continue\n",
    "                idx = len(self._word_to_id)\n",
    "                else:\n",
    "                    if word == '<UNK>':\n",
    "                        self._unk=idx\n",
    "                    else:\n",
    "                        self._word_to_id[word] = idx\n",
    "        \n",
    "    def id_for_word(word):\n",
    "        return self._word_to_id.get(word,self.unk)\n",
    "\n",
    "    def sentence_to_id(self,sentence):\n",
    "        word = [id_for_word(word) for word in sentence.split(' ')]\n",
    "        return word\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "class CategoryDict():\n",
    "    def __init__(self,filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename , 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                label = line.strip('\\r\\n')\n",
    "                idx = len(self._category_to_id)\n",
    "                self._category_to_id[label]=idx\n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "    \n",
    "    def catgory_to_id(self,label):\n",
    "        if not label in self._category_to_id:\n",
    "            raise Exception(\"the label is not exits in the text\")\n",
    "        else:\n",
    "            return self._word_to_id[label]\n",
    "\n",
    "#获得词表对象\n",
    "vocab = Vocab(vocab_file,hps.num_word_threshold)\n",
    "#获得类别对象\n",
    "catgory = CategoryDict(catgory_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextDataSet():\n",
    "    def __init__(self,filename,vocab,catgory_vocab,num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._catgory_vocab = catgory_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._indicator = 0\n",
    "        self._input =[]\n",
    "        self._output=[]\n",
    "        self.sparefile(filename)\n",
    "    def sparefile(filename):\n",
    "        with open(filename,'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            with line in lines:\n",
    "                label,content = line.strip('\\r\\n').split('\\t')\n",
    "                id_label = self._catgory_vocab.catgory_to_id(label)\n",
    "                id_word = self._vocab.sentence_to_id[content]\n",
    "                id_word = id_word[0:self._num_timesteps]\n",
    "                paddingsize = self._num_timesteps - len(id_word)\n",
    "                id_word = id_word+[self._vocab.unk for i range(paddingsize)]\n",
    "                self._input.append(id_word)\n",
    "                self._output.append(id_label)\n",
    "            self._input = np.asarray(self._input,tf.int32)\n",
    "            self._output = np.asarray(self._output,tf.int32)\n",
    "            self.random_shuffle()\n",
    "            self.num_sample = len(self._input)\n",
    "    \n",
    "    def random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._input))\n",
    "        self._input = self._input[p]\n",
    "        self._output = self._output[p]\n",
    "    \n",
    "    def next_batch(self,batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.num_sample:\n",
    "            self.random_shuffle\n",
    "            self._indicator = 0\n",
    "            self.end_indicator = self._indicator + batch_size\n",
    "        \n",
    "        if end_indicator > self.num_sample:\n",
    "            raise Exception(\"batch size is larger than samples\")\n",
    "        \n",
    "        batch_input = self._input[self._indicator:end_indicator]\n",
    "        batch_ouput = self._output[self._indicator:end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return (batch_input,batch_ouput)\n",
    "    \n",
    "train_dataset = TextDataSet(train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "val_dataset = TextDataSet(val_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(test_file, vocab, category_vocab, hps.num_timesteps)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def creat_model(hps,vorcab_size,num_class):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32,[batch_size,hps.num_timesteps])\n",
    "    outputs = tf.placeholder(tf.int32,[batch_size])\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    global_step = tf.Variable(tf.zero([],tf.int64),name='global_step',trainable=False)\n",
    "    \n",
    "#embedding\n",
    "    init_embedding = tf.random_normal(-1.0,1.0)\n",
    "    with tf.variable_op_scope('embedding',initializer=init_embedding):\n",
    "        matrix_embedding = tf.get_variable('embedding',[vorcab_size,hps.num_embedding_size])\n",
    "        embedding_input = tf.nn.embedding_lookup(matrix_embedding,inputs)\n",
    "#lstm\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_op_scope('lstm',initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in hps.num_lstm_layers:\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(hps.num_lstm_nodes[i],state_is_tuple=True)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell,output_keep_prob = keep_prob)\n",
    "            cells.append(cell)\n",
    "        \n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "        initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "        \n",
    "        rnn_output = tf.nn.dynamic_rnn(cell,embedding_input,initial_state=initial_state)\n",
    "        last = rnn_outputs[:, -1, :] ?????\n",
    "#fc \n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer = fc_init): # initializer 此范围内变量的默认初始值\n",
    "        fc_1 = tf.layers.dense(last,hps.num_fc_nodes,activation=tf.nn.relu,name='fc_1')\n",
    "        fc_drop_out = tf.nn.dropout(fc_1,keep_prob=keep_prob)\n",
    "            \n",
    "        logit = tf.layers.dense(fc_drop_out,num_class)\n",
    "    \n",
    "    with tf.name_scope('softamx'):\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=outputs,logits=logit)\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        \n",
    "        y_pre = tf.argmax(tf.nn.softmax(logit),1)\n",
    "        crre = tf.equal(y_pre,outputs)\n",
    "        \n",
    "        acc = tf.reduce_mean(tf.cast(crre,tf.float32))\n",
    "    with tf.name_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grad,_ = tf.clip_by_global_norm(tf.gradients(loss,tvars),hps.clip_lstm_grads)\n",
    "        train\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grad,tvars),global_step = global_step)\n",
    "    \n",
    "    return ((inputs, outputs, keep_prob),\n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step))\n",
    "\n",
    "\n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes\n",
    ")\n",
    "\n",
    "inputs, outputs, keep_prob = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "\n",
    "num_train_steps = 100000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_input,batch_output = train_dataset.next_batch(hps.batch_size)\n",
    "        output_val = sess.run([loss,acc,train_op,global_step],\n",
    "                              feed_dict={input:batch_input,\n",
    "                                        outputs:batch_output,\n",
    "                                         keep_prob:train_keep_prob_value}\n",
    "                             )\n",
    "    loss_val, accuracy_val, _, global_step_val = output_val\n",
    "    if i % 200 ==0:\n",
    "        print('Step: %5d, loss: %3.3f, accuracy: %3.3f' %(global_step_val, loss_val, accuracy_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
